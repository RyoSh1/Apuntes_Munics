# Tema 0: Repaso probabilidad

### Una sola variable aleatoria

#### Probabilidad en masa

La PMF es una funci√≥n que asigna a cada valor posible de la variable aleatoria discreta una probabilidad, la probabilidad de la que la variable tome ese valor espec√≠fico.

#### Funci√≥n de densidad de probabilidad

Esta funci√≥n describe la densidad de la probabilidad en un rango de valores (integral).

#### Distribuci√≥n de Laplace

Es una distribuci√≥n de probabilidad cont√≠nua similar a la normal, se usa en situaciones donde es probable observar valores externos con m√°s frecuencia.

#### Operador de esperanza

Se trata del promedio ponderado de todos los valores que puede tomar una variable aleatoria ponderados por sus respectivas probabilidades.

### M√°s de una variable aleatoria

#### Bayes

El teorema de Bayes es una f√≥rmula que describe la probabilidad de que un evento ocurra, basado en el conocimiento previo de condiciones relacionadas con el evento.

P(A|B) = P(B|A)*P(A) / P(B)

#### Independencia estad√≠stica

Dos variables son independientes si el resultado de una no tiene efecto sobre el resultado de la otra.

# Tema 1: Privacidad

## Privacidad

Habilidad de un individuo o grupo de ocultar a elecci√≥n su propia informaci√≥n. Definici√≥n y alcance subjetivo.

### Privacidad y seguridad

Seguridad como medio de privacidad.

#### Coincidencias

- Existencia de adversarios estrat√©gicos
- Principios de dise√±o de seguridad aplican privacidad.

#### Diferencias

- Trascendencia del dominio digital.
- Modelo de peligro (actores d√©biles y adversarios fuertes).
- No se puede asumir la existencia de terceros de confianza.
- Susceptibilidad a efecto de gobiernos o empleadores.

### Ataques de inferencia

A partir de datos es sencillo inferir m√°s datos, debido a que somos predecibles.

## Ejemplo de Facebook

Estudio realizado en 2013 por Michal Kosinski que se centra en como los likes de facebook se pueden usar para inferir informaci√≥n sensible y rasgos de personalidad de los usuarios. El modelo predijo a partir de datos que parec√≠an inofensivos caracter√≠sticas como orientaci√≥n sexual, rasgos de personalidad, afinidad pol√≠tica y religi√≥n; 10 likes eran suficientes para hacer inferencias precisas.

Conclusiones: La privacidad no depende solo de lo que compartimos expl√≠citamente sino de como los datos pueden combinarse y analizarse.

## Cambridge Analytica

Este caso expuso como datos masivos de usuarios fueron recolectados y utilizados sin consentimiento para influir en decisiones pol√≠ticas. El prop√≥sito de este caso fue obtener perfiles psicol√≥gicos detallados de millones de votantes, identificar preferencias pol√≠ticas, miedos y vulnerabilidades y usar todos estos datos para dise√±ar campa√±as de publicidad pol√≠tica personalizada para influir en elecciones y referendos clave.

#### Deanonimizar

- **Data Linking**: V√≠nculo con datos externos, cuanto m√°s dispersos (cantidad de atributos) m√°s sencillo de vincular.

## Ejemplo Deanonimizaci√≥n Netflix

Este se trata de un famoso ejempo de como los dato supuestamente an√≥nimos pueden vincularse a individuos usando informaci√≥n externa.

En 2006 Netflix lanz√≥ un desaf√≠o p√∫blico para mejorar su algoritmo de recomendaciones y publicaron un conjunto de datos que conten√≠a valoraciones de pel√≠culas, fechas e IDs y se encontraban anonimizados (ni nombres ni identificadores directos).

A trav√©s de datos auxiliares como valoraciones p√∫blicas en IMDb los atacantes descubrieron patrones coincidentes entre las valoraciones y las fechas, logrando reidentificar personas con pocas valoraciones p√∫blicas incluso con discrepancia en las fechas. Los usuarios que calificaban pel√≠culas sensibles quedaron expuestos sin su conocimiento.

#### Principios de GPDR

- Licitud, imparcialidad y transparencia para el interesado.
- Limiticaci√≥n de la finalidad: tratar los datos con los fines especificados expl√≠citamente.
- Minimizaci√≥n de los datos: Solo procesar los datos absolutamente necesarios.
- Exactitud
- Limitaci√≥n de almacenamiento: Solo el tiempo necesario para el fin especificado.
- Integridad y confidencialidad: Garantizada la seguridad, etc de los datos.
- Rendicion de cuentas: El responsable debe poder demostrar el cumplimiento con el GDPR.

# Tema 2: Ataques de reconstrucci√≥n de bases de datos

### Consultas de adversario

La denegaci√≥n de respuestas puede revelar datos de la propia base, si denegamos un dato importante, pero permitimos el siguiente, el atacante sabr√° informaci√≥n sobre la BD.

*Curated: Datos cuidadosamente seleccionados y con ruido a√±adido.

Los ataques de inferencia buscan reconstruir una base de datos curada.

### Curar respuestas

Responder a una consulta con una respuesta cierta viola la privacidad, por ello la respuesta debe ser una versi√≥n con ruido. Pero en orden de preservar la utilidad se debe controlar la distorsi√≥n.

## Caso Ficticio US Census Bureau Data

Caso presentado en 2018 por Simson Grafinkel que ilustra como los datos anonimizados del censo de los EEUU pueden ser reidentificados utilizando t√©cnicas estad√≠sticas modernas y datos auxiliares (en este caso se usaron datos ficticios).

Los censos recopilan informaci√≥n sensible de la poblaci√≥n como edad, g√©nero, ingresos, nivel educativo,etc. Estos datos pueden ser explotados si se identifica a individuos. Para proteger la privacidad estos datos se publican de forma anonimizada eliminando identificadores directos como nombres o direcciones.

Los investigadores utilizaron datos auxiliares para realizar ataque de reidentificaci√≥n, correlacionando atributos como edad, g√©nero, c√≥digo postal, etc. Estas combinaciones pueden ser √∫nicas para un individuo en √°reas rurales o peque√±as.

Se demostr√≥ que incluso con datos anonimizados si estos presentan combinaciones √∫nicas, se pueden utilizar c√°lculos como la media sobre poblaciones de datos pequ√±as para extraer informaci√≥n de individuos.

### Ataques de reconstrucci√≥n linear

1. Enviar todas las posibles consultas y guardar las respuestas.
2. Encontrar candidatos v√°lidos.
3. Propiedades a satisfacer.
4. Consulta particular.
5. Desigualdad del tri√°ngulo inverso.
6. Razonamiento inverso.
7. Diferencias totales.

Corolario: A menos que haya un l√≠mite en las consultas de la base de datos es posible realizar una reconstrucci√≥n casi perfecta en 4E entradas. E es el m√°ximo error que introduces en la respuesta. Reduciendo la precisi√≥n es estad√≠sticamente posible reconstruir el vector secreto con muchas menos consultas.

#### Explicaci√≥n Personal 

s - Query
a(s) - Respuesta de s
r(s) - Versi√≥n con ruido de a(s)
Para mantener utilidad |r(s) - a(s)| =< E
E - Es el l√≠mite
d - Vector secreto
c - Candidatos a vectores secretos
c^(k^t) - Un candidato que cumple que el error el menor que E (definitivo).

Teorema: Si un analista hace 2^n queries, podr√° reconstruir la BD en todo menos 4E posiciones.

Como funciona: Tu tienes 2^n queries y 2^n candidatos de ser el vector secreto, por cada candidato compruebas todas las queries, si cumple todas, tienes el vector secreto d. Lo que consigues es una columna secreta, donde cada fila cumple o no cumple con tu query [1,0,0,1 ,1,1...]. 

### Ataques de reconstrucci√≥n linear probabilisticos

Consideran la distribuci√≥n del ruido al evaluar candidatos, rechazando si respuestas asociadas tienen baja probabilidad bajo las distribuciones asumidas.

### Aircloak Diffix Challenge

Caso dise√±ado para evaluar la robustez de un sistema de anonimizaci√≥n basado en la privacidad diferencial llamado Diffix.

La soluci√≥n se bas√≥ en introducir consultas "aleatorias" mientras que llevaban a cabo pocas condiciones, por ejemplo con 3 condiciones el ruido estaba bajo O(/n), esto garantizaba un ataque exitoso bajo la condici√≥n de O(n) ganando el premio.

# Introducci√≥n a la Privacidad diferencial

Escenario inicial: Un dataset D contiene una fila por cada usuario y un curador produce una salida R a partir de aplicar un mecanismo M al dataset.

A trav√©s de un estudio de los datos e informaci√≥n externa un atacante puede encontrar correlaciones entre los datos y obtener informaci√≥n sensible. La privacidad diferencial se encarga de proteger los datos , de tal forma que observar R no debe proporcionar m√°s conocimiento del que se tiene previamente.

### Encontrar Diferencias

El mecanismo M debe ser capaz de producir un R de dos bases de datos (una le falta una fila) que sean indistinguibles. El dise√±o del mecanismo debe ser probabil√≠stico.

Las distribuciones de ambas salidas deben ser similares, esto debe mantenerse para todos los posibles datasets vecinos (1 fila diferente).

Distribuciones similares: Introducimos un par√°metro p como umbral de diferencia para valorar la privacidad, este par√°metro se transforma en e elevado a epsilon para mejorar los c√°lculos.

## Configuraci√≥n de la privacidad diferencial

Dependiendo de donde se ejecuta el mecanismo, existen dos modelos de privacidad diferencial:

- Central: Hay un agregador centralizado.
- Local: Cada usuario ejecuta el mecanismo por si solo.

La definici√≥n es id√©ntica, pero la local se tiene en cuenta como si los datasets tuvieran solo una fila.

DP limitado: Los conjuntos de datos vecinos tienen el mismo n√∫mero de registros y difieren en el valor de una fila. Se usa cuando todos est√°n presentes en ambos conjuntos.

DP No-Limitado: Los conjuntos vecinos difieren en la inclusi√≥n o exclusi√≥n de una fila. Se usa cuando alguno individuos no estan presentes en alguno de los conjuntos.

## Mis propios apuntes de explicaci√≥n

PR() representa la probabilidad de que un evento espec√≠fico ocurra. Pr[M(D) ‚àà S] es la probabilidad de que el mecanismo M, aplicado al conjunto de datos D, produzca un resultado que est√© dentro del conjunto de resultados S.

#### Ejemplo pr√°ctico de la privacidad

Si e=1 y el valor real es 50 (enfermos), entonces M(x) puede ser cualquier numero cercano a 50, pero con menos probabilidad confirme se aleja de este valor. 

Si el ruido generado sigue una distribuci√≥n Laplaciana con media 0 y escala b = 1/e = 1, entonces:

La probabilidad de que M(x) = 51 es:

1/2b exp(-|51 - 50| / b) = 1/2 exp(-1) == 0.184

La probabilidad de x = 50 es 0.5

Perr >= 1 / (1 + e^e) = 0.26

### Parametro epsilon

Es un par√°metro que controla el nivel de privacidad en un mecanismo de DP. Valores peque√±os = Mayor privacidad (menos info a la salida), Valores grandes = Menos privacidad (expone m√°s informaci√≥n).

Un mecanismo M satisface Œµ-DP si para cada conjunto de salidas S y cualquier par de conjuntos de datos vecinos D y D', se cumple que Pr1/Pr2 =< e^e

e^(epsilon) es una cota superior multiplicativa que relaciona las distribuciones de las salidas se un mecanismo sobre dos conjuntos de datos vecinos.

Ejemplo: Si epsilon = 1 , e = 2.718, lo que significa que la probabilidad de una salida no cambia m√°s de un factor de ~2.7, independientemente de la presencia o ausencia de un individuo.

#### DP como un juego estad√≠stico

Si un adversario intenta distinguir dos conjuntos de datos vecinos utilizando las salidas del mecanismo M, epsilon proporciona un l√≠mite sobre la capacidad del adversario para tomar decisiones correctas.

Una e-DP peque√±a asegura que la probabilidad de error del adversario es alta, lo que equivale a m√°s privacidad.

### Problemas con e-DP

Garantizar e-DP estricta puede ser complicado debido al ruido necesario para preservar la privacidad, para ello se utilizan mecanismos aproximados (relajaci√≥n) como (Œµ,Œ¥)-DP que permiten una peque√±a probabilidad Œ¥ de fallar en cumplir Œµ-DP.

## Mecanismos de privacidad diferencial

#### Mecanismo de Laplace

A√±ade ruido Laplaciano proporcional a la sensibilidad de la funci√≥n f y el par√°metro de privacidad epsilon. La l-sesibilidad es el m√°ximo cambio que puede haber a la salida al cambiar D por D'.

F√≥rmula: M(x) = f(x) + Lap(Œîf/Œµ)

Donde Œîf es la sensibilidad de f, es decir, el cambio m√°ximo en la salida al modificar una sola fila en el conjunto de datos.

Ejemplo: Calculamos la sensibilidad ( max1 - max2) y dividimos eso entre nuestro valor de e. Por ejemplo para sensibilidad de 180 y e=0.1 -> Lap(1800).

#### Mecanismo de Respuesta aleatoria

Ideal para entradas binarias, responde con la verdad con probabilidad p y miente con probabilidad 1-p. Garantiza Œµ-DP ajustando p adecuadamente.

#### Mecanismo Exponencial

Dise√±ado para seleccionar entre resultados discretos bas√°ndose en una funci√≥n de utilidad. La probabilidad de elegir un resultado est√° ponderada exponencialmente por su utilidad.

u es la utilidad y Œîu es su sensibilidad.

#### Mecanismo Gaussiano

Similar al de Laplace, pero a√±ade ruido Gaussiano. √ötil para aproximaciones de DP ((ùúÄ,ùõø)-DP).

## Resumen Mecanismos


| Ataque   | Descripci√≥n | Ventaja | Desventaja |
|----------|----------|----------|----------|
| Mecanismo Laplaciano | A√±ade ruido Laplaciano (distribuci√≥n de Laplace) proporcional a la sensibilidad de la query y el par√°metro epsilon | Sencillo, eficiente en consultas con baja sensibilidad y no requieren grandes ajusts de ruido y garantiza epsilon-DP | Menos efectivo contra datos de alta dimensi√≥n, si la sensibilidad es alta puede degradar la calidad de los resultados |
| Mecanismo de respuesta aleatoria | Los usuarios introducen ruido directamente en sus respuestas antes de enviar datos al recolector, encuestas sensibles | Privacidad descentralizada, Adaptable y ligero de implementar  | Puede presentar ruido muy alto, escalabilidad limitada |
| Mecanismo exponencial  | Selecciona un elemento de un conjunto de opciones con probabilidad proporcional a su utilidad, a√±adiendo ruido a trav√©s de la distribuci√≥n exponencial | Optimizado para elecciones discretas y permite priorizar resultados m√°s √∫tiles mientras mantiene privacidad | Limitaci√≥n en queries continuas y requiere una sensibilidad compleja |
| Mecanismo Gaussiano | A√±ade ruido Gaussiano (distribuci√≥n normal) proporcional a la sensibilidad de la query y al par√°metro de privacidad | Escalabilida, flexible y adaptado a la privacidad aproximada (Œ¥) | No garantiza la privacidad diferencial estricta sino aproximada, el ajuste de ruido puede ser m√°s complejo |

IMPORTANTE VER LOS EJEMPLOS

## Propiedades de DP

### Robustez al Post-procesamiento

Si un mecanismo M satisface e-DP, cualquier funci√≥n derivada o transformaci√≥n basada en su salida tambi√©n lo satisface. Esto implica que la privacidad no se reduce al realizar c√°lculos adicionales, por lo que un adversario no puede revertir el ruido agragado.

### Privacidad de grupo

Si un mecanismo satisface e-DP para conjuntos de datos que difieren en una sola fila, garantiza k-e-DP para conjuntos que difieren en k filas.

Proporciona una cota sobre la p√©rdida de privacidad para grupos de individuos en lugar de 1.

### Composici√≥n Secuencial

Si se ejecutan m√∫ltiples mecanismos independientes, el mismo conjunto de datos, la privacidad combinada es la suma de e. Cada consulta disminuye la privacidad, por lo que es importante limitarlo.

### Composici√≥n Paralela

Si diferentes mecanismos operan sobre peticiones disjuntas de los datos, la privacidad combinada es igual a la mayor e de ellos.

Permite realizar m√∫ltiples operaciones en paralelo sin afectar significativamente la privacidad.

# Cifrado Homom√≥rfico

### Cifrado homom√≥rfico

Hacer ejemplo

### Lattices

Una lattice n-dimensional es una combinaci√≥n de enteros de n vectores base. Se usan en sistemas criptogr√°ficos resistentes contra computadoras cu√°nticas.

### Problemas dif√≠ciles en lattices

Problema del vector m√°s corto (Encontrar la normal del vector m√°s corto en la lattice), aproximaci√≥n Œ± de SVP y el problema de SVP independientes.

### Problema de aprendizaje con errores

Es un problema matem√°tico de la teor√≠a de lattices que es fundamental en sistemas resistentes a los ataques de computadoras cu√°nticas, se utiliza para dise√±ar esquemas criptogr√°ficos como el cifrado, las firmas digitales y el cifrado homom√≥rfico.

#### Nota respecto a la reducci√≥n modular

Las transformaciones en m√≥dulo q seguir√°n la siguiente distribuci√≥n de intervalos:
- Para n√∫meros impares: [-(q-1)/2, (q-1)/2]
- Para n√∫meros pares: [-q/2, q/2-1]

#### B√∫squeda LWE < Decisi√≥n LWE

- B√∫squeda: Dado poly(n) pares (ai,bi), buscar el vector s.
- Decisi√≥n: Dado poly(n) pares (ai,bi), decidir:
    - Han sido generados como (ai,bi = S(T) ai + ei mod q)...
    - Han sino generados como (ai,bi) donde a es muestra uniforme de Z(n,q) y b es muestra de Zq

Resolver decisi√≥n implica resolver b√∫squeda.

### Error

El cigrado homom√≥rfico y los problemas LWE tienen un rango de contenci√≥n del error, dependiendo de como sea la constante en operaciones como multiplicaci√≥n, es necesario mantener el error controlado para que la salida del descifrado no quede corrupta.

### Cifrado sim√©trico usando LWE



### Cifrado homom√≥rfico

Es una t√©cnica criptogr√°fica que permite realizar c√°lculos directamente sobre datos cifrados, sin necesidad de descifrarlos primero. Los resultados cuando se descifran son los mismos que si se hubieran realizado sobre los datos originales. Los cifrados pueden ser Parcialmente Homom√≥rficos, Levemente homom√≥rficos o Totalmente homom√≥rficos.

### A√±adir dos mensajes



### Suma o resta desde una constante



### Multiplicaci√≥n



### Descomposici√≥n Gadget



### Cambio de claves





# Salto Parte 2

# T√©cnicas de anonimidad

## Introducci√≥n

La anonimizaci√≥n de datos es un mecanismo utilizado para sanitizar informaci√≥n con el objetivo de proteger la privacidad de las personas referenciadas en los datos. Esto implica tratar la informaci√≥n de identificaci√≥n personal (PII) de manera que se evite su divulgaci√≥n evitando riesgo de fugas de informaci√≥n al compartir datos con terceros o al hacerlos p√∫blicos. Existen regulaciones como GPDR o HIPAA que exigen estos procedimientos para proteger datos personales.

Elementos que deben ser anonimizados: 

- Identificadores: Atributos que identifican de forma √∫nica individuos.
- Cuasi-identificadores: Atributos que combinados pueden identificar individuos.
- Atributos sensibles: No deben vincularse con una persona.
- No sensibles: Irrelevantes en el an√°lisis.

Enfoques comunes para la anonimizaci√≥n:

- Enmascaramiento de datos: Los datos se ocultan o alteran para evitar que los originales sean reconstruidos (cifrado, mezcla, sustituci√≥n, caracteres). Se puede realizar cambio est√°tico (al replicar BD) o din√°mico (al consultar).
- Pseudoanonimizaci√≥n: Sustituir identifiacores con pseudonimos artificiales, se mantiene un v√≠nculo interno para revertir.
- Generalizaci√≥n: Reemplaza valores espec√≠ficos con rangos o categor√≠as, se necesita volumen de datos para garantizar ambig√ºedad sin perder utilidad.
- Intercambio: Permutaciones o mezcla entre filas de una misma columna.
- Perturbaci√≥n de datos: Introducci√≥n de ruido o redondeo que cambia los datos, pero mantiene estad√≠sticas.
- Datos sint√©ticos: En lugar de compartir datos reales se genera a trav√©s de modelos generativos datos sint√©ticos que imita los originales.

### K-Anonimidad

Un dataset es K-anonimo cuando existen K registros con el mismo cuasi-identificador (set de atributos), el valor de k determina la utilidad de los datos, a mayor k m√°s generalizaci√≥n de datos.

Consideraciones:

- Los Cuasi-identificadores y los atributos sensibles deben ser distinguidos adecuadamente para que no revelen informaci√≥n sobre los atributos anonimizados.
- Los datos de un grupo deben estar diversificados para que no queden filas identificables.
- Es importante considerar la dimensionalidad de los datos, deben estar ni muy separados ni ser muy similares.

Ejemplo: Grupo [edad, c√≥digo Postal] debe aparecer id√©ntico en K filas diferentes.

### L-Diversidad

Extensi√≥n de K que determina que un grupo k-anonimo debe tener L distintos registros sensibles para hacerlo m√°s robusto en privacidad. Puede no ser eficaz si un valor dominante representa la mayor√≠a.

Ejemplo: grupo [edad, c√≥digo Postal], atributo sensible enfermedad debe tener L valores.

### T-Closeness

Mejora la L-diversidad asegurando que un atributo sensible tenga una distribuci√≥n similar a la global. Se define un umbral T para valorar la distancia entre la distribuci√≥n de un atributo sensible en un grupo.

Ejemplo: Si hay un 60% de personas saludables, en un subgrupo debe haber 60 +- T de distribuci√≥n.

### Algoritmo de Mondrian

Algoritmo para aplicar la K-Anonimidad, la idea es realizar una partici√≥n multidimensional en los cuasi-identificadores para generar regiones distintas.

Por cada regi√≥n:

- Los atributos num√©ricos se codifican usando rangos m√°ximos y m√≠nimos.
- Los atributos categ√≥ricos se define un conjunto que los represente.

### Privacidad de Geolocalizaci√≥n

La geolocalizaci√≥n es un tema clave debido al auge de los servicios basados en ubicaci√≥n (LBS), pero estos aunque √∫tiles generan riesgos de privacidad: Revelar actividades o h√°bitos personales, es necesario implementar mecanismos de privacidad para minimizar riesgos.

- Datos de localizaci√≥n: Caracter√≠sticas que distinguen a una persona (identidad), Informaci√≥n espacial (coordenadas, nombres de lugares o proximidad f√≠sica) e Informaci√≥n temporal (timestamps para rastrear movimiento).
- Desaf√≠os: Correlaciones entre datos de posici√≥n, trayectoria y tiempo pueden comprometer la privacidad. Los datos en tiempo real son muchos y exigen procesamiento inmediato. 

Ataques y riesgos: 

1. M√©todos de ataque:
    - Acceso a informaci√≥n p√∫blica o semip√∫blica.
    - Interceptar tr√°fico real.
    - Comprometer servidores o dispositivos.
2. Tipos de ataque:
    - Ataques de identidad para revelar PII.
    - Rastreo de ubicaci√≥n pasada o presente.
    - Inferir relaciones entre individuos o grupos.
    - Cruzar contexto de bases de datos con informaci√≥n de ubicaci√≥n.
3. Impacto: An√°lisis de rutinas o predicci√≥n de comportamientos y revelaci√≥n de puntos sensibles de inter√©s (POIs).

M√©todos de Preservaci√≥n:

1. Criptograf√≠a:
    - Claves compartidas para cifrar datos.
    - Servicios federados para distribuir la informaci√≥n.
2. Anonimizaci√≥n:
    - T√©cnicas como K-anonimidad, L o T para evitar identificar individuos espec√≠ficos.
    - Uso de zonas mixtas, √°reas donde los pseud√≥nimos cambian para evitar rastreos.
3. Obfuscaci√≥n:
    - Introducir ubicaciones falsas (dummies).
    - Representaci√≥n de √°reas en vez de coordenadas exactas.
    - Uso de ruido aleatorio para alterar ubicaciones.

# Introducci√≥n al Machine Learning

### Inteligencia Artificial

La inteligencia artificial tiene como objetivo desarrollar sistema que demuestren la flexibilidad y versatilidad de la inteligencia humana para resolver un amplio abanico de problemas complejos (general).

El objetivo espec√≠fico es desarrollar un sistema que es capaz de solucionar un problema para el cual ha sido dise√±ado.

## Machine Learning

Rama de la IA cuyo objetivo es desarrollar algoritmos de aprendizaje para m√°quinas, desarrolla modelos computacionales capaces de aprender a solucionar problemas complejos a partir de ejemplos. 

Es adecuado cuando no sabes como montar un algoritmo que solucione un problema, pero si tienes ejemplos de la soluci√≥n.

### Tipos de aprendizaje

Existen muchos tipos con sus subtipos, diferenciamos como principales el supervisado, no supervisado y refuerzo.

Dentro del aprendizaje supervisado tenemos problemas de clasificaci√≥n (etiqueta) y problemas de Regresi√≥n (valor cont√≠nuo como la edad).

#### Generalizaci√≥n y sobreentrenamiento

La generalizaci√≥n es el t√©rmino usado para describir la capacidad de un modelo de predecir o clasificar nuevos datos.

Sobreentrenamiento define un modelo que ha sido entrenado demasiado bien, provocando que memorice los datos de entrenamiento, pero no sea capaz de predecir nuevos datos.

#### Preparaci√≥n de datos

Antes de usar datos para entrenar un modelo es necesario prepararlos y "limpiarlos": 
- Normalizaci√≥n.
- Recodificaci√≥n de valores no num√©ricos.
- Eliminaci√≥n de ruido.
- Imputaci√≥n de datos.

## Modelos lineales de aprendizaje supervisado

### Notaciones matem√°ticas y definiciones

Escalares (*x,y*), Vectores (**x,y**), Matrices (**X,Y**), Productos, Normales de vectores.

### M√©todos de regresi√≥n lineal

El mecanismo de regresi√≥n se basa en predecir el valor de una o varias variables continuas dado el valor de un conjunto de variables explicativas representadas por un vector m-dimensional.

- Elementos:
    - Variables explicativas: Par√°metros de entrada del modelo.
    - Ejemplos de entrenamiento: Set de n datos de las variables explicativas cuya predicci√≥n es conocida.
    - Modelo: Funci√≥n parametrizada que representa la relaci√≥n entre x y t (w).
    - Objetivo (error o coste): Funci√≥n que indica como de bien se aproximan los datos entrenados.
    - M√©todo de optimizaci√≥n que encuentra los par√°metros √≥ptimos minimizando la funci√≥n objetivo.

- Proceso de entrenamiento:
    - El objetivo es construir un modelo que encuentre los par√°metros √≥ptimos **w** para predecir el valor **t** de un nuevo valor **x**.
    - Esto requiere un set de datos de entrenamiento con n observaciones y valores predecibles.

#### Regresi√≥n linear

La regresi√≥n lineal se define como:  
~~~
y(x,w) = w0 + w1x1 + . . . + wmxm = xTw.
~~~

El error para cada i se calcula como:
~~~
ei = ti ‚àí yi(xi ,w)
~~~

#### M√≠nimos cuadrados

La funci√≥n de error (objetivo) se define como la suma del cuadrado de los errores. (convexa)

### M√©todos de clasificaci√≥n lineal

La clasificaci√≥n es similar a la regresi√≥n, salvo que el valor a predecir se encuentra en un conjunto peque√±o de valores discretos. En la clasificaci√≥n binaria solo tenemos 2 valores disponibles.

Se le llama supervisado porque los valores de salida se conocen en todo momento.

Tipos de problemas: Separables linearmente, No separables linearmente y Datos no separables.

#### Decisi√≥n de regiones

- M√©todos de aprendizaje lineal: Las superficies de decisi√≥n que generan son funciones lineales (hiperplanos).
- M√©todos de aprendizaje no lineal: Las superficies que generan son funciones no lineales.

#### Regresi√≥n log√≠stica

Para resolver el problema del m√©todo de m√≠nimos cuadrados, la regresi√≥n log√≠stica propone el uso de una funci√≥n log√≠stica o sigmoidea. Esto obliga a que la salida est√© acotada en el intervalo (0,1), lo que tambi√©n permite que pueda interpretarse como una probabilidad.

Para obtener el m√≠nimo del error es necesario utilizar m√©todos iterativos basados en el gradiente descendiente.

#### Conclusiones

M√≠nimos cuadrados no es una buena solucion en general para clasificaci√≥n porque es sensible a clases desbalanceadas.

Regresi√≥n log√≠stica es el m√©todo m√°s robusto.

## M√©tricas para evaluaci√≥n de errores

¬øC√≥mo podemos evaluar el rendimiento de un modelo? La funcion de p√©rdidas es la que utilizamos para optimizar el modelo. Las m√©tricas de evaluaci√≥n se encargan de medir el rendimiento.

### M√©tricas para regresi√≥n

- M√≠nimos cuadrados (MSE): Otorga mayor peso a errores grandes al usar el cuadrado.
- Ra√≠z de los m√≠nimos cuadrados(RMSE): Facilita la interpretaci√≥n porque el error obtenido es relativo a las unidades de los datos.
- Media del error absoluto(MAE): Trata todos los errores por igual, pero no es adecuada si quieres prestar m√°s atenci√≥n a errores potencialmente grandes.
- Media porcentual del error absoluto(MAPE):
    - Interpretaci√≥n clara e independiente de la escala de los datos, resistente al error at√≠pico.
    - Problemas con la divisi√≥n en valores iguales a 0 o generaci√≥n de n√∫meros muy grandes si los valores reales son excepcionalmente peque√±os. Sesgado a predicciones menores a los valores reales.
- Media porcentual sim√©trica del error absoluto(SMAPE):
    - Sim√©trica respecto a valores grandes o peque√±os y soluciona el problema de MAPE porque genera valores entre 0% y 100%.
    - Mantiene los problemas de dividir entre 0 y no trata sim√©tricamente las sobre- y sub- predicciones.

### M√©tricas para clasificaci√≥n supervisada

- Precisi√≥n: Tasa de acierto global del sistema.
- Sensibilidad: Tasa de acierto de los positivos.
- Especificaci√≥n: Tasa de acierto de los negativos.
- Exactitud: Tasa de acierto de positivos sobre todos los clasificados como positivos.
- F1-score: Media arm√≥nica entre precisi√≥n y sensibilidad.
- ROC curve: Representa gr√°ficamente la sensibilidad frente el ratio de falsos positivos.
- Area bajo ROC curve (AUC): La AUC indica como de bien est√°n separadas las posibilidades de clases positivas y negativas.

En el caso de multiclases la precisi√≥n se calcula sobre el resultado global y el resto en el contexto de la propia clase.

- Macro-media: Media de la m√©trica sobre todas las clases.
- Media ponderada: Media de la m√©trica pero ponderada por el n√∫mero de casos por clase.
- Micro-media: Por cada m√©trica los casos individuales de verdaderos positivos y falsos negativos y positivos se acumulan para cada clase.

## Metodolog√≠as para el an√°lisis de resultados

Como podemos evaluar de forma realista las m√©tricas, el objetivo es la generalizaci√≥n del error del modelo. Para ello tenemos varios m√©todos de estimaci√≥n del error.

### Partici√≥n simple

Dividir el dataset en dos subsets (entrenamiento y test). Si los datos son pocos es un lujo usar eso como test, solo se realiza un test, por lo que el resultado no es adecuado.

### Sub ejemplos aleatorios

Realizar K experimentos usando diferentes subsets de los datos, cada subset se elige de forma aleatoria y el resto para entrenamiento. El estimador del error real se consigue con la media de los errores de los K experimentos.

### K-fold cross-validation

Se divide el dataset en K subsets disjuntos de aproximadamente el mismo tama√±o, con ellos se realizan K experimentos usando cada subset como test y el resto como entrenamiento. El error de validaci√≥n es calculado como la media de cada E.

#### Stratified K-fold cross-validation

Variante de K-fold donde cada set contiene aproximadamente el mismo numero de casos de cada clase del data set completo.

#### Dejar uno fuera

Caso extremo de K-fold donde se realizan N experimentos con N datos, dejando siempre uno solo para test.

### Cross-Validation

¬øCuantos subsets y experimentos se deben realizar?

- Si se elige un numero grande: El estimador de error va a ser preciso, la varianza del error va a ser alta y el tiempo computacional va a ser alto.
- Si se eligen pocos: Tiempo computacional reducido, Varianza peque√±a y estimador impreciso.

## Modelos no lineales de aprendizaje supervisado

Permiten la creaci√≥n de zonas complejas de decisi√≥n para separar datos de distintas clases (suelen funcionar mejor que los lineales).

### Redes de neuronas artificiales

Las neuronas se organizan en una serie de capas que define la arquitectura de la red. El entrenamiento se basa en encontrar los pesos √≥ptimos a trav√©s de funciones de error como MSE o Cross-entropy. M√©todos de gradiente descendiente se utilizan para minimizar la funci√≥n de error.

#### Back propagation

La back-propagation de error es un m√©todo eficiente de calcular el gradiente necesario para optimizar los pesos en una red multicapa.

1. Con un bloque de datos de entrenamiento propaga la salida hacia delante para calcular el error.
2. Propaga hacia atr√°s el error para obtener el gradiente en cada peso.
3. Usa los gradientes para actualizar los pesos. 

#### Algoritmos avanzados

Adam, RMSprop, AdaGrad, Momentum.

#### Decisi√≥n de regi√≥n

Si la red no tiene capas ocultas la regi√≥n es un hiperplano, si tiene 1 capa oculta es convexa, con 2 capas se convierte en una combinaci√≥n de regiones convexas

#### Capacidad representacional

Seg√∫n el teorema de aproximaci√≥n universal de G.Cybenko una red de una capa puede representar cualquier funci√≥n continua, una sola capa puede ser masiva para adaptarse a los datos. Pero lo normal es que la arquitectura sea m√°s profunda.

### Redes neuronales convolucionales

Un modelo de red profundo que es capaz de capturar de forma correcta las dependencias temporales y espaciales de una imagen aplicando filtros. Esta arquitectura es m√°s adecuada para im√°genes debido a la reducci√≥n del n√∫mero par√°metros usados y reuso de pesos. Adecuada para el tipo rejilla.

#### Capa convolucional

Es una capa con una serie de filtros (kernels) que se aplican en la imagen y se pueden aprender. Estos filtros se activan cuando se detecta una caracter√≠stica visual como bordes o colores, el objetivo es extraer caracter√≠sticas relevantes de la imagen. Esencialmente realiza el producto escalar entre filtros y regiones locales de la imagen.

Normalmente tiene m√°s de una capa convolucional, la primera es la responsable de capturar caracter√≠sticas de bajo nivel y el resto de alto.

#### Capa RELU

Normalmente usada tras una capa convolucional para transformar la operaci√≥n lineal que esta realiza, es una funci√≥n de activaci√≥n definida como max(0,x).

#### Capa de Pooling

Se encarga de reducir el tama√±o de la salida de la capa convolucional. Esto ayuda a reducir el n√∫emero de par√°metros y controlar as√≠ el sobreentrenamiento. Lo normal es colocar esta capa entre capas convolucionales sucesivas.

Tipos de pooling:
    - M√°ximos: Devuelve el valor m√°ximo de la parte cubierta por el filtro.
    - Media: Devuelve la media de todos los valores parte de la imagen cubierta por el filtro.

#### Capa completamente conectada

Esta capa permite aprender combinaciones no lineales de caracter√≠sticas de alto nivel dadas por la capa convolucional. La entrada a esta capa es transformada en plano a un vector columna, durante el proceso de entrenamiento es capaz de distinguir caracter√≠sticas dominantes y clasificarlas usando softmas.

#### Recomendaciones para un buen entrenamiento

- Normalizar los datos (desviaci√≥n est√°ndar por p√≠xel).
- Aumentar los datos (rotando im√°genes).
- Usar datos balanceados en todas las clases.
- Aleatorizar el orden de entrada en entrenamiento.
- Inicializar los pesos aleatoriamente.
- Usar un set de validaci√≥n para evitar sobreentrenamiento.

# Aprendizaje federado

El aprendizaje federado es un enfoque colaborativo y distribuido para entrenar modelos de aprendizaje autom√°tico sin necesidad de compartir los datos originales de los usuarios.

#### Caracter√≠sticas principales

Los datos permanecen en los dispositivos locales, lo que mitiga riesgos de filtraci√≥n o uso indebido. Es especialmente √∫til en situaciones donde los datos est√°n dispersos o no pueden centralizarse debido a restricciones legales.

#### Diferencias con enfoques tradicionales

En lugar de recopilar datos en una plataforma centralizada, los nodos locales realizan el entrenamiento parcial del modelo. Los par√°metros actualizados se comparten con un servidor central que coordina el entrenamiento y construye un modelo global.

### Descripci√≥n del m√©todo

- Flujo de trabajo: 
    1. El servidor coordina la inicializaci√≥n del modelo y distribuye los par√°metros a los nodos. 
    2. Los nodos son seleccionados para una ronda de entrenamiento y el modelo es compartido por el coordinador.
    3. Cada nodo entrena el modelo con sus datos locales y env√≠a actualizaciones al servidor. 
    4. El coordinador recibe actualizaciones entrenadas y las agrega para mejorar el modelo global.
    5. Esto genera una nueva versi√≥n actualizada del modelo que repite los pasos anteriores.
    6. Si el modelo cumple un criterio de rendimiento se termina y se produce el modelo final.

- Datos no uniformes: Los datos en los nodos pueden estar distribuidos de manera desigual, lo que presenta desafios en t√©rminos de convergencia y precisi√≥n del modelo.
- Algoritmos optimizados: Equilibran eficiencia y precisi√≥n, aunque pueden ser menos efectivos en comparaci√≥n con enfoques centralizados.


### Problemas de privacidad y Modelos de ataque

- Ataques de inferencia:
    - Ataques de inversi√≥n del modelo: Recuperar datos de entrenamiento a partir del modelo.
    - Ataques de inferencia de membres√≠a: Determinar si un registro espec√≠fico est√° en el conjunto de datos de entrenamiento o de alguno de los nodos.
- Tipos de ataque: Desde el cliente comprobando las versiones del modelo o desde el servidor comprobando las actualizaciones enviadas.
- Ataques de envenenamiento:
    - Los adversarios manipulan actualizaciones del modelo para desviarlo hacia una soluci√≥n sub√≥ptima o inyectar modelos con puertas traseras.

### Mecanismos de Preservaci√≥n de la privacidad

- Prevenci√≥n de ataques de inferencia:
    - Cifrado homom√≥rfico que permite entrenar modelos sobre datos cifrados.
    - Agregaci√≥n segura usando computaci√≥n multipartida para combinar actualizaciones sin exponer datos individuales.
    - Privacidad diferencial a√±adiendo ruido a los datos o modelos.
- Prevenci√≥n de ataques de envenenamiento:
    - T√©cnicas de detecci√≥n de anomal√≠as en modelos.
    - Inspecci√≥n de datos y actualizaciones enviadas por los participantes.

# Privacy Preserving Machine Learning

Privacy Enhancing Tools (PET) con el enfoque en aprencizaje autom√°tico.

El aprendizaje autom√°tico preservando la privacidad (PPML) busca proteger los datos sensibles durante el entrenamiento y uso de modelos. Sin embargo, estos sistemas tienen varias amenazas que comprometen tanto los datos como los modelos.

## Modelos de amenazas

- Actores implicados: 
    - Propietarios de datos sensibles.
    - Propietarios y consumidores del modelo.
    -  Adversarios interesados en explotar vulnerabilidades.
- Superficies de ataque: 
    - Durante el entrenamiento cuando el modelo se construye.
    - Durante la inferencia, uso del modelo entrenado.
    - Comportamiento del atacante: Pasivo (observador), Activo (Manipula el proceso de entrenamiento o inferencia).
- Tipos de ataques: Basados en conocimiento limitado o completo del modelo y datos.

## Tipos de ataques

- Inferencia sobre miembros de la poblaci√≥n
    - Revelaci√≥n estad√≠stica
    - Inversi√≥n de modelos
    - Inferencia de representantes de clase
- Inferencia sobre los miembros del conjunto de datos de entrenamiento.
    - Inferencia de membres√≠a.
    - Inferencia de propiedades.
- Inferencia sobre par√°metros del modelo.
    - Extracci√≥n del modelo (precisi√≥n de tarea).
    - Robo de funcionalidades.

### Ataque de inferencia de membres√≠a

El objetivo es determinar si una entrada ha sido usado en el entrenamiento basandose en el comportamiento del modelo sobre datos no conocidos o datos de entrenamiento.

- Caja negra: asume conocimiento sobre la predici√≥n de salida.
- Caja blanca: tiene acceso a par√°metros y gradientes.
- Contra generativos: Obtiene informaci√≥n sobre los datos de entrenamiento usando conocimiento sobre los componentes de generaci√≥n de datos.
- Contra Federados: Deducir si un registro espec√≠fico forma parte de cualquier participante.

- Es necesario cuantificar la fuga de informaci√≥n de miembros a trav√©s de los resultados de predicci√≥n del modelo.
    - Dado un modelo y un registro determinar si se utiliz√≥.
    - Investigar dado el entorno m√°s dificil -> Caja negra.
- Un problema de inferencia de membres√≠a es un problema de clasificaci√≥n.
    - Entrenar un modelo para distinguir el comportamiento entre entrenamiento y no.
    - Usar Shadow Training: Shadow models que imitan el comportamiento del modelo objetivo.
    - Entrenar el modelo de ataque en las entradas y salidas etiquetadas de los Shadow.

#### Generar datos para los modelos Shadow

- S√≠ntesis de modelo: El atacante no tiene ni datos ni estad√≠sticas, por lo que genera datos usando el modelo v√≠ctima, los datos generados con gran confianza deber√≠an ser similares a los de entrenamiento.
- S√≠ntesis estad√≠stica: El atacante puede tener infomaci√≥n estad√≠stica sobre la poblaci√≥n de entrenamiento, cada muestra tiene su propia distribuci√≥n marginal.
- Datos reales ruidosos: El atacante debe tener acceso a datos similares a los usados en el entrenamiento de la v√≠ctima.

#### Entrenar el modelo de ataque

Para el entrenamiento se hacen consultas a los modelos shadow usando un set de entrenamiento/test disjunto, se etiquetan las salidas como in/out respectivamente, se crean particiones por cada etiqueta diferente (diferentes modelos de ataque) y el resultado es una clasificaci√≥n binaria

#### Resultados del ataque de membres√≠a

Los ataques son robustos incluso si las suposiciones de distribuci√≥n de entrenamiento del atacante no son precisas, para la mayor parte de los ataques es posible entrenarse solo con conocimiento de caja negra.

Para modelos del mismo tipo el sobreentrenamiento hace que sea m√°s vulnerable, otros problemas pueden ser la estructura, la generalizaci√≥n y la diversidad de los datos de entrenamiento.

### Ataques de reconstrucci√≥n

Los ataques de reconstrucci√≥n intentan recrear muestras de entrenamiento o sus etiquetas, puede ser parcial o completa y los datos creados pueden ser los ratos originales o representativos de las propiedades sensibles.

#### Inversi√≥n de modelos

Una alta generalizaci√≥n puede producir una alta probabildidad de inferir atributos de los datos, un gran poder de predicci√≥n es m√°s susceptible a los ataques de reconstrucci√≥n.

- M. Fredrikson contra LR: El adversario no tiene acceso al modelo ni conocimiento sobre caracter√≠sticas. Utiliza estimaci√≥n de probabilidad m√°xima a posteriori (MAP) para inferir los valores de las caracter√≠sticas sensibles, maximizando la probabilidad de observar par√°metros conocidos.
- S.Hidano sobre LR: No se asumen conocimientos, se basa en la posibilidad de realizar un ataque de envenenamiento durante el entrenamiento para influir en las predicciones.
- M. Fredrikson contra MLP/Autoencoders: Formulado como un problema de optimizaci√≥n, el objetivo es usar el gradiente descendiente para recuperar datos de entrada que coincidan con las salidas observadas.

#### Inferencia de propiedades

Estos ataques buscan extraer propiedades del conjunto de datos de entrenamiento que no est√°n expl√≠citamente codificadas como caracter√≠sticas ni est√°n relacionadas con la tarea principal del modelo. Permiten identificar vulnerabilidades en el sistema y posibilitan la creaci√≥n de modelos similares al objetivo.

Perspectiva del adversario: Las propiedades inferidas pueden ser generales o espec√≠ficas, este tipo de ataques puede ocurrir incluso en modelos bien generalizados.

### Ataques de extracci√≥n de modelo

El objetivo es potencialmente la construcci√≥n de un modelo sustituto que imita al original a trav√©s de la extracci√≥n de informaci√≥n. Principalmente el objetivo de los modelos de sustituci√≥n es:
- Extracci√≥n de la precisi√≥n para igualar la del modelo original en los test.
- Extracci√≥n de la fidelidad: igualar un conjunto de puntos de entrada no relacionados y crear una imitaci√≥n.

No es estrictamente necesario conocer la arquitectura del modelo v√≠ctima mientras el sustituto tenga la misma o mayor complejidad, el objetivo es robar hiperpar√°metros y propiedades de la arquitectura.

Causas: El subentrenamiento aumenta el √©xito del ataque, los modelos con mayor generalizaci√≥n o mayor n√∫mero de clases son m√°s dificiles de atacar.

- Extracci√≥n de precisi√≥n de tarea: inferir par√°metros del modelo, 
    - Elegir entradas √∫tiles: Elegir entradas sint√©ticas cerca del l√≠mite de decisi√≥n del modelo objetivo. Otras estrategias ser√≠a no utilizar datos sint√©ticos de otros dominios, tecnicas semisupervisadas o generar entradas aleatorias.

- Extracci√≥n de funcionalidad:
    - T.Orekondy: Basado en pares entrada/salida observados en consultas MLaaS, interactua con CNN de caja negra proveyendo imagenes de entrada y obteniendo predicciones, estas predicciones se utilizan para entrenar el modelo de imitaci√≥n.
    - N.Papernot: La v√≠ctima es un DNN, los inpues los genera un adversario y se etiquetan por el DNN.

## Resumen de los ataques GPT

| Ataque   | Objetivo Principal | Ventaja | Desventaja |
|----------|----------|----------|----------|
| Ataque de inferencia de Membres√≠a  | Determinar si un dato espec√≠fico fue parte del entrenamiento.En clasificaci√≥n usar Shadow Models.  | Detecta vulnerabilidades en t√©rminos de privacidad   | Solo indica si un dato estuvo en el entrenamiento, no qu√© datos |
| Inversi√≥n de modelo    | Recuperar datos sensibles o privados a partir del modelo | Revela vulnerabilidades en cuanto a datos sensibles  | Requiere un modelo accesible y puede no ser f√°cil de ejecutar |
| Inferencia de propiedades  | Inferir caracter√≠sticas estad√≠sticas del conjunto de datos  | Detecta patrones generales en el modelo entrenado | Menos preciso que otros ataques y puede no revelar datos espec√≠ficos |
|  Extracci√≥n de modelo  | Obtener una aproximaci√≥n del modelo entrenado | Permite replicar el modelo o usarlo sin acceso directo | La precisi√≥n depende de la cantidad de consultas y puede no replicar completamente el modelo |

# PPML Parte 2

## Privacidad diferencial

Las t√©cnicas de privacidad diferencial resisten los ataques de inferencia de membres√≠a a√±adiendo ruido aleatorio a los datos de entrada, a la iteraciones del algoritmo de machine learning y a las salidas del algoritmo.

- Input: Se a√±ade en la entrada del modelo, tras el entrenamiendo de machine learning la salida ser√° diferancialmente privada. Requiere a√±adir m√°s ruido al input porque los datos tienen mayor sensibilidad.
- Perturbaci√≥n de algoritmo: Aplicado a modelos que utilizan varias iteraciones, se a√±ade ruido en los c√°lculos internos del algoritmo, requiere un dise√±o distinto para cada algoritmo. Tiene menos sensitividad en los datos e introduce menos ruido.
- Perturbaci√≥n objetiva: Modificar la funci√≥n objetiva de aprendizaje, esto cambia el problema de optimizaci√≥n dificultando la extracci√≥n de informaci√≥n.
- Perturbaci√≥n de salida: Usar un algoritmo de aprendizaje no privado y a√±adir ruido al modelo generado, antes de ser compartido o utilizado.

### Privacidad diferencial local

Los individuos env√≠an sus datos al agregador tras privatizar los datos con las perturbaciones.

Mecanismos LDP: 
- Una dimensi√≥n: Respuesta aleatoria y estimaci√≥n de frecuencia.
- Multidimensional: Laplace, Duchi, Piecewise.

### Generaci√≥n de datos sint√©ticos para preservar la privacidad

- Necesidad de datos en ML: Los algoritmos de aprendizaje autom√°tico requieren grandes vol√∫menes de datos para alcanzar su m√°ximo rendimiento. A menudo no es factible recopilar y compartir datos reales en cantidades suficientes debido a restricciones legales.
- Datos Sint√©ticos como Soluci√≥n: Los datos sint√©ticos son generados artificialmente mediante algoritmos que imitan la distribuci√≥n y las propiedades de los datos originales. Conservan caracter√≠sticas cr√≠ticas de los datos reales y permiten resultados similares, incluso en escenarios raros o poco comunes. Garantizan la protecci√≥n de la privacidad mientras se mantiene la utilidad de los datos para aplicaciones espec√≠ficas. El objetivo es inferir las etiquetas a partir de los registros y las salidas.

#### M√©todos de generaci√≥n y preservaci√≥n de privacidad

- Anonimizaci√≥n de datos: K-Anonimidad protege contra reidentificaci√≥n y L-Diversidad evita problemas con la homogeneidad de datos sensibles en los grupos
- Privacidad diferencial: Introduce ruido aleatorio para proteger la informaci√≥n sensible mientras se generan: 
    - Histogramas sint√©ticos. 
    - Datos tabulares sint√©ticos (tablas completas)
    - Datos multimarginales (distribuciones complejas con privacidad garantizada).

### T√©cnicas de minado de datos para preservar la privacidad

Minado de datos: Herramientas y t√©cnicas que se pueden usar cada vez que la informaci√≥n recogida es procesada y analizada para extraer conocimiento.

- Modelos descriptivos: Identificar relaciones entre datos y descripciones reconocibles por humanos.
- Modelos prescriptivos: Usados para predecir el futuro basandose en el pasado.

# Comunicaciones an√≥nimas

Las redes de comunicaci√≥n utilizan direcciones visibles para enrutar datos, estas direcciones suelen ser identificadores √∫nicos, lo que compromete la privacidad al asociarse con personas f√≠sicas.

La anonimizaci√≥n protege tanto la privacidad del usuario como las comunicaciones frente a an√°lisis de tr√°fico. Se extiende a t√©cnicas como autenticaci√≥n an√≥nima, votaci√≥n y transacciones electr√≥nicas.

Niveles de anonimato: Privacidad total (nadie identifica a nadie), Privacidad parcial (Las agencias de seguridad pueden romper el anonimato bajo √≥rdenes judiciales) y sin anonimato.

Conceptos clave seg√∫n Pfitzmann y Hansen:

- Anonimato: Estado en el que un individuo no puede ser identificado dentro de un grupo. Requiere que haya sujetos con atributos iguales.
- No vinculaci√≥n: Imposibilidad de relacionar m√∫ltiples usos de un servicio por un mismo usuario.
- No observabilidad: Los mensajes son indistinguibles del ruido aleatorio.
- Pseudoanonimato: Uso de un pseud√≥nimo como identificador √∫nico, pero vinculado al usuario.

Modelos de ataque en redes:

- Atacante pasivo (Tipo 1): Observa los enlaces de comunicaci√≥n.
- Atacante pasivo con env√≠o (Tipo 2): Emite mensajes.
- Atacante activo (Tipo 3): Controla enlaces, puede borrar, retrasar o modificar mensajes.

Requisitos para el anonimato en redes:

- Tr√°fico de cobertura: A√±adir tr√°fico adicional para ocultar transmisiones reales. Si el atacante controla este tr√°fico, el anonimato queda comprometido.
- Tr√°fico embebido: Los mensajes de los usuarios deben integrarse silenciosamente en el tr√°fico de cobertura. Requiere una tercera parte confiable que combine mensajes reales y falsos.
- Efectividad: Definida como la proporci√≥n de mensajes reales respecto al total, requiere coordinar suficientes usuarios y mensajes para minimizar retrasos y maximizar la eficiencia.

## Redes Mixtas

Redes de muchos nodos donde el paquete a la salida es una permutaci√≥n de las entradas, mientras su contenido se mantiene inalterado. Si al menos un nodo oculta su shuffling la permutaci√≥n es secreta,la honestidad de los nodos puede verificarse p√∫blicamente garantizando que las salidas son una permutaci√≥n de las entradas.

Una red mixta debe funcionar aun en el supuesto de que un nodo falle o se vea comprometido.

### Modos de procesamiento

Cadena de descifrado: Cada mensaje es cifrado secuencialmente utilizando la clave p√∫blica de cada nodo, concatenando Address || Bloque || Random string. 

Desencriptaci√≥n parcial: Cada nodo utiliza su clave privada para quitar una capa de encriptaci√≥n del mensaje recibido, que luego se reordena de manera aleatoria antes de enviarse a la siguiente etapa.

Permutaci√≥n: Los mensajes parcialmente desencriptados se mezclan en un orden aleatorio y se transmiten como un lote al siguiente nodo, asegurando que no se pueda rastrear la relaci√≥n entre entrada y salida.

Para garantizar la comunicaci√≥n bidireccional an√≥nima se incluye informaci√≥n de ruta de retorno (RPI) y claves compartidas sim√©tricas con el mensaje original. Cada nodo desencripta una capa de esta informaci√≥n revelando la siguiente direccion y clave.

Cadena de reencriptaci√≥n: En este modelo los nodos no desencriptan los mensajes sino que los reencriptan usando cadenas aleatorias para cambiar su apariencia antes de enviarlos. Al final de todo se realiza una fase de desencriptaci√≥n conjunta usando un esquema de claves compartidas.

Existen variaciones de estas redes utilizando esquemas h√≠bridos que combinan claves sim√©tricas y asim√©tricas.

### Topolog√≠as

- Cascada: 
    - Consiste en una secuencia fija de etapas, com√∫n para todos los remitentes y receptores. El primero nodo inicial la mezcla de mensajes en lotes de tama√±o l, procesandolos de forma as√≠ncrona. 
    - Desventajas: Una etapa defectuosa puede comprometer toda la red, es posible rastrear mensajes mediante an√°lisis de tr√°fico (pasivo), Controlar varias etapas o inyectar tr√°fico (ataque activo).
- Enrutamiento libre: 
    - Las etapas estan interconectadas pero no dependen unas de otras. Un nodo puede recibir remitentes y enviar salidas directamente a otros nodos, la operaci√≥n es as√≠ncronas, pueden esperar a formar lotes o ser enviados m√°s r√°pido seg√∫n su ruta.
    - los mensajes pueden rastrearse debido al tr√°fico no uniforme y al tama√±o decreciente de las "capas" de cifrado (pasivo), inundar la red con tr√°fico puede aislar mensajes objetivo (activo).

### Esquemas de verificaci√≥n

- Criterios para verificar una red mixta: Los lotes de entrada se procesan correctamente y se permutan de forma adecuada. Los mensajes no deben ser alterados, a√±adidos ni eliminados.
- Niveles de verificaci√≥n: Salidas de toda la red, salidas de cada etapa individual. Estos m√©todos no suelen aplicarse en topolog√≠as de enrutamiento libre.

Esquemas espec√≠ficos de verificaci√≥n:

- Red mixta verificable por el remitente: Detecta mensajes corruptos solo en la salida de la red, el remitente incluye un checksum cifrado con su clave privada. Cualquiera puede verificar la integridad desencriptando con la clave p√∫blica del remitente.
No detecta mensajes a√±adidos, detecta mensajes eliminados solo si el remitente verifica la presencia de su mensaje y no identifica etapas comprometidas.
- Red mixta verificable por etapas: Cada etapa verifica sus salidas usando subprotocolos adicionales.
M√©todos: Crear copias del lote de entrada o repetir el proceso de mezcla, Pruebas de conocimiento cero o revelaci√≥n controlada de secretos para validad operaciones, Mecanismos de recuperaci√≥n para reiniciar operaciones si se detecta un comportamiento an√≥malo.

## Onion Routing

El Onion Routing se basa en el enrutamiento multinodo y cifrado en m√∫ltiples capas. Cada mensaje se cifra capa por capa con las claves de los nodos del camino hacia el receptor, cada nodo desencripta una capa para revelar informaci√≥n sobre el siguiente nodo y reenv√≠a el mensaje.

Funcionamiento: No mezcla ni reordena como las redes mixtas, el orden de entrada/salida no es relevante.

- Entidades: Cliente, Onion Proxy (determina la ruta y construye la cebolla), Nodos intermedios y Nodos finales.

Respuesta: Los nodos a√±aden capas de cifrado en sentido inverso para enviar una respuesta al remitente.

### TOR

Tor es una mejora de Onion Routing, los usuarios se conectan a trav√©s de una aplicaci√≥n (no es una red P2P), los nodos son proporcionados por voluntarios que donan ancho de banda y procesamiento.

Limitaciones: Vulnerable a ataques extremos si el atacante controla los nodos finales, identificando remitente, receptor y contenido. No oculta la identidad del remitente a nivel aplicaci√≥n.

Construcci√≥n: Siempre se usan 3 nodos(in, mid, out), el de entrada es confiable y elegido por su estabilidad, los circuitos rotan peri√≥dicamente y se usan claves a corto y largo plazo para negociar cifrados y proteger datos. La clave a largo plazo se usa como identificador y la de corto plazo como clave onion.

Transferencia de datos: Los mensajes se dividen en paquetes fijos de 512B, los routers desencriptan una capa en cada salto y en las respuestas se hace el proceso inverso.

Servicios ocultos: 
- Permite que usuario y servidor no conozcan sus direcciones IP mutuas, el servidor selecciona nodos cebolla como "puntos de introducci√≥n" y publica un descriptor en una tabla hash distribuida (pk y direcciones de los puntos).
- El usuario elige un punto de introducci√≥n y un "punto de encuentro" aleatorio, se solicita a ese punto que introduzca cliente y servidor y si el servidor acepta se crea un circuito para conectarse de forma segura.